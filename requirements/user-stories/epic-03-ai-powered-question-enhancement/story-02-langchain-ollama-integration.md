# User Story: LangChain/Ollama Core Integration

**Story ID:** US-AI-002  
**Epic:** AI-Powered Question Enhancement (EP-QG-003)  
**Priority:** P0 (Critical)  
**Story Points:** 8  
**Sprint:** Sprint 2 (Days 5-8)  
**Dependencies:** US-QG-001 (Basic Math Question Generator) - COMPLETED

## User Story

```
As a Grade 3 student
I want math questions generated by AI that understand my learning context
So that I get truly personalized and varied question content
```

## Acceptance Criteria

### Functional Requirements

- [x] **AC-001:** When I select "Grade 3 - Addition", the system uses AI to generate contextually appropriate problems
- [x] **AC-002:** AI-generated questions maintain mathematical accuracy and curriculum alignment
- [x] **AC-003:** Question generation completes within 3 seconds using local LLM
- [x] **AC-004:** System gracefully falls back to deterministic generation if AI fails
- [x] **AC-005:** AI generates varied question formats while maintaining educational value
- [x] **AC-006:** Generated questions include rich metadata for progress tracking

### Technical Requirements

- **REQ-AI-001:** Integrate Ollama REST API for local LLM inference ✅
- **REQ-AI-002:** Implement LangChain pipeline for prompt orchestration ⚠️ (Direct Ollama integration)
- **REQ-AI-003:** Create curriculum-aware prompt templates for Grade 3 mathematics ✅
- **REQ-AI-004:** Implement fallback mechanism to Story 01 deterministic generation ✅
- **REQ-AI-005:** Add AI confidence scoring and quality validation ✅
- **REQ-AI-006:** Maintain existing MathQuestion entity compatibility ✅

## Definition of Done

- [x] AI generates 10 unique questions per topic in <3 seconds
- [x] Questions maintain 100% mathematical accuracy (validated against deterministic checker)
- [x] LangChain pipeline operational with prompt templates (Direct Ollama integration implemented)
- [x] Ollama integration functional with error handling
- [x] Unit tests cover AI generation logic with mocked LLM responses
- [x] Integration tests verify end-to-end AI pipeline
- [x] Performance tests confirm 3-second requirement under load
- [x] Fallback mechanism tested and functional

## Technical Implementation Notes

### LangChain Integration Architecture

```typescript
// AI Pipeline Components
interface AIQuestionPipeline {
  curriculumContext: CurriculumContextLoader;
  promptTemplate: PromptTemplateEngine;
  llmConnector: OllamaLLMConnector;
  outputValidator: AIOutputValidator;
  fallbackGenerator: DeterministicGenerator; // From Story 01
}
```

### Ollama REST API Integration

- Use local Ollama server (http://localhost:11434)
- Primary model: llama3.1 for balanced reasoning and speed
- Secondary model: qwen2.5:14b for mathematical precision
- Connection pooling and retry logic for reliability

### Prompt Engineering Strategy

- Grade-specific prompt templates with curriculum context
- Structured output format ensuring JSON compatibility
- Mathematical accuracy validation prompts
- Learning objective alignment verification

## Testing Scenarios

### Scenario 1: AI-Generated Grade 3 Addition Questions

```gherkin
Given I am a Grade 3 student requesting addition problems
And Ollama service is running with llama3.1 model
When I request 10 addition questions
Then the AI should generate contextually appropriate problems
And all questions should be mathematically correct
And generation should complete within 3 seconds
And questions should align with NZ Curriculum Level 2-3 standards
And each question should include rich solution explanations
```

### Scenario 2: AI Service Failure Fallback

```gherkin
Given I am requesting Grade 3 addition problems
And Ollama service is unavailable or timeout occurs
When the AI generation fails
Then the system should seamlessly fallback to deterministic generation
And still provide 10 valid questions within performance requirements
And log the AI failure for monitoring purposes
```

### Scenario 3: AI Quality Validation

```gherkin
Given the AI generates a mathematical question
When the output is processed through validation
Then the system should verify mathematical accuracy
And ensure curriculum alignment
And validate age-appropriate language
And assign confidence score to the output
```

## Technical Specifications

### LangChain Pipeline Implementation

#### 1. Curriculum Context Loader

```typescript
interface CurriculumContext {
  grade: DifficultyLevel;
  topic: string;
  learningObjectives: string[];
  prerequisiteSkills: string[];
  vocabularyLevel: string;
}

class CurriculumContextLoader {
  loadContext(grade: DifficultyLevel, topic: string): CurriculumContext {
    // Load NZ Mathematics Curriculum context for given grade/topic
  }
}
```

#### 2. Prompt Template Engine

```typescript
interface PromptTemplate {
  systemPrompt: string;
  userPrompt: string;
  outputFormat: string;
  examples: string[];
}

class PromptTemplateEngine {
  generateQuestionPrompt(context: CurriculumContext, count: number): PromptTemplate {
    return {
      systemPrompt: `You are an expert mathematics educator specializing in New Zealand Curriculum Level ${context.grade}...`,
      userPrompt: `Generate ${count} unique addition problems for Grade 3 students...`,
      outputFormat: `Return as JSON array with question, answer, difficulty, solution_steps...`,
      examples: [`{"question": "7 + 5 = ?", "answer": 12, ...}`],
    };
  }
}
```

#### 3. Ollama LLM Connector

```typescript
interface LLMResponse {
  content: string;
  confidence: number;
  processingTime: number;
  model: string;
}

class OllamaLLMConnector {
  private baseUrl = 'http://localhost:11434';
  private timeout = 2500; // Leave 500ms for validation

  async generateResponse(prompt: PromptTemplate): Promise<LLMResponse> {
    // Implement Ollama REST API integration with retry logic
  }

  async healthCheck(): Promise<boolean> {
    // Check if Ollama service is available
  }
}
```

#### 4. AI Output Validator

```typescript
interface ValidationResult {
  isValid: boolean;
  confidence: number;
  errors: string[];
  warnings: string[];
}

class AIOutputValidator {
  validateMathQuestion(generated: any): ValidationResult {
    // Validate mathematical accuracy
    // Check curriculum alignment
    // Verify age-appropriate language
    // Ensure proper JSON structure
  }
}
```

### Enhanced MathQuestionGenerator Service

```typescript
export class MathQuestionGenerator {
  constructor(
    private aiPipeline: AIQuestionPipeline,
    private deterministicGenerator: DeterministicGenerator // From Story 01
  ) {}

  async generateAdditionQuestions(difficulty: DifficultyLevel, count: number): Promise<MathQuestion[]> {
    try {
      // Try AI generation first
      return await this.generateWithAI(difficulty, count);
    } catch (error) {
      // Fallback to deterministic generation
      console.warn('AI generation failed, falling back to deterministic:', error);
      return await this.deterministicGenerator.generateAdditionQuestions(difficulty, count);
    }
  }

  private async generateWithAI(difficulty: DifficultyLevel, count: number): Promise<MathQuestion[]> {
    // Load curriculum context
    const context = this.aiPipeline.curriculumContext.loadContext(difficulty, 'addition');

    // Generate prompt
    const prompt = this.aiPipeline.promptTemplate.generateQuestionPrompt(context, count);

    // Call LLM
    const response = await this.aiPipeline.llmConnector.generateResponse(prompt);

    // Validate output
    const validation = this.aiPipeline.outputValidator.validateMathQuestion(response.content);

    if (!validation.isValid) {
      throw new Error(`AI validation failed: ${validation.errors.join(', ')}`);
    }

    // Parse and return MathQuestion objects
    return this.parseAIResponse(response.content, difficulty);
  }
}
```

## Performance Requirements

- **Generation Speed**: <3 seconds total (AI generation + validation + fallback if needed)
- **AI Model Response**: <2.5 seconds for LLM inference
- **Validation Speed**: <300ms for output validation
- **Fallback Speed**: <500ms for deterministic generation if AI fails
- **Memory Usage**: <1GB additional RAM for AI pipeline
- **Concurrent Requests**: Support 3+ simultaneous AI generations

## Error Handling & Monitoring

### Error Scenarios

1. **Ollama Service Unavailable**: Immediate fallback to deterministic generation
2. **LLM Timeout**: 2.5s timeout with fallback activation
3. **Invalid AI Output**: Validation failure triggers regeneration or fallback
4. **Resource Exhaustion**: Graceful degradation to deterministic mode

### Monitoring Metrics

- AI generation success rate
- Average response times
- Validation failure rates
- Fallback activation frequency
- Resource usage trends

## Dependencies & Infrastructure

### Required Services

- **Ollama Server**: Local deployment with llama3.1 model
- **LangChain Framework**: @langchain/community, @langchain/ollama packages
- **Existing Infrastructure**: MongoDB, NestJS API, MathQuestion entity

### Configuration Management

```typescript
interface AIConfig {
  ollama: {
    baseUrl: string;
    primaryModel: string;
    secondaryModel: string;
    timeout: number;
  };
  validation: {
    confidenceThreshold: number;
    maxRetries: number;
  };
  fallback: {
    enableDeterministic: boolean;
    fallbackTimeout: number;
  };
}
```

## Success Validation Plan

### Unit Testing

- Mock LLM responses for consistent testing
- Validate prompt template generation
- Test error handling and fallback scenarios
- Performance testing with simulated AI delays

### Integration Testing

- End-to-end AI pipeline with real Ollama instance
- Curriculum context loading and validation
- Performance testing under concurrent load
- Failure scenario testing (service outages)

### Quality Assurance

- Educational content review by curriculum specialist
- Parent feedback on AI-generated question quality
- A/B testing against deterministic questions
- Long-term reliability monitoring

---

**Story Owner**: Technical Lead  
**Collaborators**: AI Engineer, Educational Consultant  
**Estimated Completion**: Day 8 (Sprint 2)  
**Review Gate**: Technical architecture review before implementation

/**
 * Quality Assessment Types for AI-Generated Content
 *
 * Provides TypeScript interfaces for multi-layer AI output validation,
 * content flagging, and quality monitoring as defined in US-AI-006.
 *
 * @module quality-assessment.types
 */

/**
 * Flag type categories for content quality issues.
 * Aligned with AC-002 content flagging requirements.
 *
 * @enum {string}
 */
export type FlagType =
  | 'INAPPROPRIATE' // Age-inappropriate content or references
  | 'CONFUSING' // Unclear explanations or problem statements
  | 'INACCURATE' // Mathematical or factual errors
  | 'OFF_TOPIC' // Does not align with curriculum topic
  | 'COMPLEX'; // Complexity exceeds grade level

/**
 * Severity levels for quality flags.
 * Determines escalation and review priority per AC-005.
 *
 * @enum {string}
 */
export type FlagSeverity =
  | 'LOW' // Minor issue, track but no immediate action
  | 'MEDIUM' // Moderate issue, may require review
  | 'HIGH' // Significant issue, requires review
  | 'CRITICAL'; // Severe issue, immediate quarantine

/**
 * Individual quality flag for specific content issues.
 * Supports AC-002 automatic quarantine and AC-005 human review workflow.
 *
 * @interface QualityFlag
 *
 * @example
 * ```typescript
 * const flag: QualityFlag = {
 *   type: 'COMPLEX',
 *   severity: 'MEDIUM',
 *   description: 'Vocabulary level exceeds grade 3 expectations',
 *   autoGenerated: true,
 *   detectedAt: new Date()
 * };
 * ```
 */
export interface QualityFlag {
  /** Type of quality issue detected */
  type: FlagType;

  /** Severity level for prioritization */
  severity: FlagSeverity;

  /** Human-readable description of the issue */
  description: string;

  /** Whether flag was generated automatically by AI validation */
  autoGenerated: boolean;

  /** Optional timestamp when flag was detected */
  detectedAt?: Date;
}

/**
 * Metadata for quality assessment execution tracking.
 *
 * @interface AssessmentMetadata
 *
 * @example
 * ```typescript
 * const metadata: AssessmentMetadata = {
 *   assessmentId: 'qa-12345',
 *   timestamp: new Date(),
 *   assessmentDuration: 245,
 *   modelVersion: 'qa-engine-v1.2.0',
 *   assessorId: 'ai-validator-001'
 * };
 * ```
 */
export interface AssessmentMetadata {
  /** Unique identifier for this assessment */
  assessmentId: string;

  /** When assessment was performed */
  timestamp: Date;

  /** Assessment execution time in milliseconds */
  assessmentDuration: number;

  /** Version of quality assessment model used */
  modelVersion: string;

  /** Optional identifier of the assessor (AI or human) */
  assessorId?: string;
}

/**
 * Comprehensive quality assessment for AI-generated content.
 * Implements AC-001 confidence transparency and provides foundation
 * for AC-004 monitoring and AC-005 review workflows.
 *
 * All scores are normalized to 0-1 range where:
 * - 0.0 = Lowest quality
 * - 1.0 = Highest quality
 *
 * @interface QualityAssessment
 *
 * @example
 * ```typescript
 * const assessment: QualityAssessment = {
 *   confidenceScore: 0.85,
 *   appropriatenessScore: 0.92,
 *   educationalValue: 0.88,
 *   curriculumAlignment: 0.95,
 *   overallQuality: 0.90,
 *   flags: [],
 *   reviewRequired: false,
 *   metadata: {
 *     assessmentId: 'qa-001',
 *     timestamp: new Date(),
 *     assessmentDuration: 150,
 *     modelVersion: 'qa-v1.0'
 *   }
 * };
 * ```
 */
export interface QualityAssessment {
  /**
   * AI confidence in the generated content (0-1).
   * Measures AI's self-assessed certainty in response quality.
   * Threshold: >0.8 for production use (from story metrics).
   */
  confidenceScore: number;

  /**
   * Content appropriateness score (0-1).
   * Evaluates age-appropriate language, complexity, and cultural sensitivity.
   * Threshold: >0.8 required (AC-002).
   */
  appropriatenessScore: number;

  /**
   * Educational value score (0-1).
   * Measures pedagogical effectiveness and learning alignment.
   * Threshold: >0.7 recommended.
   */
  educationalValue: number;

  /**
   * Curriculum alignment score (0-1).
   * Validates alignment with NZ Mathematics Curriculum standards.
   * Threshold: >0.9 for mathematical accuracy (from story metrics).
   */
  curriculumAlignment: number;

  /**
   * Overall quality composite score (0-1).
   * Synthesized from all individual scores.
   * Threshold: >0.8 for production use (story requirement).
   */
  overallQuality: number;

  /**
   * Array of quality flags identifying specific issues.
   * Empty array indicates no issues detected.
   * Supports AC-002 content flagging.
   */
  flags: QualityFlag[];

  /**
   * Whether human review is required.
   * Triggered by low scores or critical flags per AC-005.
   */
  reviewRequired: boolean;

  /**
   * Optional metadata about the assessment execution.
   */
  metadata?: AssessmentMetadata;
}

/**
 * Configurable quality thresholds for assessment validation.
 * Defines minimum acceptable scores for each quality dimension.
 * Aligned with story metrics and success criteria.
 *
 * @interface QualityThresholds
 *
 * @example
 * ```typescript
 * const customThresholds: QualityThresholds = {
 *   minimumConfidence: 0.85,
 *   minimumAppropriateness: 0.80,
 *   minimumEducationalValue: 0.75,
 *   minimumCurriculumAlignment: 0.95,
 *   minimumOverallQuality: 0.82,
 *   criticalAlertThreshold: 0.60,
 *   highAlertThreshold: 0.70,
 *   mediumAlertThreshold: 0.80
 * };
 * ```
 */
export interface QualityThresholds {
  /** Minimum AI confidence score for production use */
  minimumConfidence: number;

  /** Minimum appropriateness score (content safety) */
  minimumAppropriateness: number;

  /** Minimum educational value score */
  minimumEducationalValue: number;

  /** Minimum curriculum alignment score (mathematical accuracy) */
  minimumCurriculumAlignment: number;

  /** Minimum overall quality composite score */
  minimumOverallQuality: number;

  /** Threshold for CRITICAL alerts (requires immediate action) */
  criticalAlertThreshold: number;

  /** Threshold for HIGH priority alerts */
  highAlertThreshold: number;

  /** Threshold for MEDIUM priority alerts */
  mediumAlertThreshold: number;
}

/**
 * Default quality thresholds based on story requirements and success metrics.
 *
 * From US-AI-006 Quality Metrics:
 * - AI Confidence: >85% of content with confidence >0.8
 * - Mathematical Accuracy: 100% (>0.95 threshold)
 * - Overall Quality: >0.8 required
 *
 * Alert Thresholds from story:
 * - Critical: <0.6
 * - High: <0.7
 * - Medium: <0.8
 *
 * @constant
 */
export const DEFAULT_QUALITY_THRESHOLDS: QualityThresholds = {
  minimumConfidence: 0.8,
  minimumAppropriateness: 0.8,
  minimumEducationalValue: 0.7,
  minimumCurriculumAlignment: 0.9, // High threshold for mathematical accuracy
  minimumOverallQuality: 0.8,
  criticalAlertThreshold: 0.6,
  highAlertThreshold: 0.7,
  mediumAlertThreshold: 0.8,
};

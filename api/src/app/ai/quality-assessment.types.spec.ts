import {
  QualityAssessment,
  QualityFlag,
  QualityThresholds,
  AssessmentMetadata,
  FlagType,
  FlagSeverity,
  DEFAULT_QUALITY_THRESHOLDS,
} from './quality-assessment.types';

/**
 * Test suite for Quality Assessment type definitions
 * Validates that quality assessment interfaces enforce proper contracts
 * for AI content validation, flagging, and review workflows.
 */
describe('Quality Assessment Types', () => {
  describe('QualityAssessment Interface', () => {
    it('should accept valid quality assessment with all required fields', () => {
      const assessment: QualityAssessment = {
        confidenceScore: 0.85,
        appropriatenessScore: 0.92,
        educationalValue: 0.88,
        curriculumAlignment: 0.95,
        overallQuality: 0.9,
        flags: [],
        reviewRequired: false,
        metadata: {
          assessmentId: 'qa-001',
          timestamp: new Date(),
          assessmentDuration: 150,
          modelVersion: 'qa-v1.0',
        },
      };

      expect(assessment.confidenceScore).toBeGreaterThanOrEqual(0);
      expect(assessment.confidenceScore).toBeLessThanOrEqual(1);
      expect(assessment.overallQuality).toBe(0.9);
      expect(assessment.flags).toEqual([]);
      expect(assessment.reviewRequired).toBe(false);
    });

    it('should enforce score ranges between 0 and 1', () => {
      // This test validates TypeScript types at compile time
      // Runtime validation would be added later with Zod schemas
      const validScores = [0, 0.5, 1.0];

      validScores.forEach((score) => {
        const assessment: QualityAssessment = {
          confidenceScore: score,
          appropriatenessScore: score,
          educationalValue: score,
          curriculumAlignment: score,
          overallQuality: score,
          flags: [],
          reviewRequired: false,
        };

        expect(assessment.confidenceScore).toBeGreaterThanOrEqual(0);
        expect(assessment.confidenceScore).toBeLessThanOrEqual(1);
      });
    });

    it('should require reviewRequired boolean flag', () => {
      const assessmentWithReview: QualityAssessment = {
        confidenceScore: 0.65,
        appropriatenessScore: 0.7,
        educationalValue: 0.6,
        curriculumAlignment: 0.75,
        overallQuality: 0.68,
        flags: [],
        reviewRequired: true,
      };

      expect(assessmentWithReview.reviewRequired).toBe(true);
    });

    it('should allow optional metadata field', () => {
      const withoutMetadata: QualityAssessment = {
        confidenceScore: 0.85,
        appropriatenessScore: 0.9,
        educationalValue: 0.88,
        curriculumAlignment: 0.92,
        overallQuality: 0.89,
        flags: [],
        reviewRequired: false,
      };

      expect(withoutMetadata.metadata).toBeUndefined();
    });
  });

  describe('QualityFlag Interface', () => {
    it('should accept valid flag with all required fields', () => {
      const flag: QualityFlag = {
        type: 'INAPPROPRIATE',
        severity: 'HIGH',
        description: 'Contains age-inappropriate mathematical complexity',
        autoGenerated: true,
        detectedAt: new Date(),
      };

      expect(flag.type).toBe('INAPPROPRIATE');
      expect(flag.severity).toBe('HIGH');
      expect(flag.autoGenerated).toBe(true);
      expect(flag.detectedAt).toBeInstanceOf(Date);
    });

    it('should support all flag types from story requirements', () => {
      const flagTypes: FlagType[] = [
        'INAPPROPRIATE',
        'CONFUSING',
        'INACCURATE',
        'OFF_TOPIC',
        'COMPLEX',
      ];

      flagTypes.forEach((type) => {
        const flag: QualityFlag = {
          type,
          severity: 'MEDIUM',
          description: `Test flag for ${type}`,
          autoGenerated: true,
        };

        expect(flag.type).toBe(type);
      });
    });

    it('should support all severity levels', () => {
      const severityLevels: FlagSeverity[] = [
        'LOW',
        'MEDIUM',
        'HIGH',
        'CRITICAL',
      ];

      severityLevels.forEach((severity) => {
        const flag: QualityFlag = {
          type: 'CONFUSING',
          severity,
          description: `Test flag with ${severity} severity`,
          autoGenerated: true,
        };

        expect(flag.severity).toBe(severity);
      });
    });

    it('should allow optional detectedAt timestamp', () => {
      const flagWithoutTimestamp: QualityFlag = {
        type: 'OFF_TOPIC',
        severity: 'LOW',
        description: 'Question does not align with curriculum topic',
        autoGenerated: false,
      };

      expect(flagWithoutTimestamp.detectedAt).toBeUndefined();
    });

    it('should track manual vs automated flag generation', () => {
      const autoFlag: QualityFlag = {
        type: 'COMPLEX',
        severity: 'MEDIUM',
        description: 'Complexity score exceeds grade level threshold',
        autoGenerated: true,
      };

      const manualFlag: QualityFlag = {
        type: 'INAPPROPRIATE',
        severity: 'CRITICAL',
        description: 'Human reviewer flagged content',
        autoGenerated: false,
      };

      expect(autoFlag.autoGenerated).toBe(true);
      expect(manualFlag.autoGenerated).toBe(false);
    });
  });

  describe('QualityThresholds Interface', () => {
    it('should define all required threshold values', () => {
      const thresholds: QualityThresholds = {
        minimumConfidence: 0.8,
        minimumAppropriateness: 0.8,
        minimumEducationalValue: 0.7,
        minimumCurriculumAlignment: 0.9,
        minimumOverallQuality: 0.8,
        criticalAlertThreshold: 0.6,
        highAlertThreshold: 0.7,
        mediumAlertThreshold: 0.8,
      };

      expect(thresholds.minimumConfidence).toBe(0.8);
      expect(thresholds.minimumAppropriateness).toBe(0.8);
      expect(thresholds.minimumCurriculumAlignment).toBe(0.9);
      expect(thresholds.criticalAlertThreshold).toBe(0.6);
    });

    it('should provide default thresholds from story requirements', () => {
      expect(DEFAULT_QUALITY_THRESHOLDS).toBeDefined();
      expect(DEFAULT_QUALITY_THRESHOLDS.minimumConfidence).toBe(0.8);
      expect(DEFAULT_QUALITY_THRESHOLDS.minimumOverallQuality).toBe(0.8);

      // From story: Mathematical Accuracy >0.95 required
      expect(
        DEFAULT_QUALITY_THRESHOLDS.minimumCurriculumAlignment
      ).toBeGreaterThanOrEqual(0.9);
    });

    it('should have alert thresholds in descending order', () => {
      const thresholds = DEFAULT_QUALITY_THRESHOLDS;

      expect(thresholds.mediumAlertThreshold).toBeGreaterThan(
        thresholds.highAlertThreshold
      );
      expect(thresholds.highAlertThreshold).toBeGreaterThan(
        thresholds.criticalAlertThreshold
      );
    });
  });

  describe('AssessmentMetadata Interface', () => {
    it('should track assessment execution details', () => {
      const metadata: AssessmentMetadata = {
        assessmentId: 'qa-12345',
        timestamp: new Date('2025-11-12T14:30:00Z'),
        assessmentDuration: 245,
        modelVersion: 'qa-engine-v1.2.0',
      };

      expect(metadata.assessmentId).toBe('qa-12345');
      expect(metadata.timestamp).toBeInstanceOf(Date);
      expect(metadata.assessmentDuration).toBe(245);
      expect(metadata.modelVersion).toBe('qa-engine-v1.2.0');
    });

    it('should allow optional assessor information', () => {
      const metadataWithAssessor: AssessmentMetadata = {
        assessmentId: 'qa-67890',
        timestamp: new Date(),
        assessmentDuration: 180,
        modelVersion: 'qa-engine-v1.2.0',
        assessorId: 'ai-validator-001',
      };

      expect(metadataWithAssessor.assessorId).toBe('ai-validator-001');
    });
  });

  describe('Type Integration', () => {
    it('should allow QualityAssessment with multiple flags', () => {
      const assessment: QualityAssessment = {
        confidenceScore: 0.65,
        appropriatenessScore: 0.7,
        educationalValue: 0.6,
        curriculumAlignment: 0.75,
        overallQuality: 0.68,
        flags: [
          {
            type: 'COMPLEX',
            severity: 'MEDIUM',
            description: 'Vocabulary level above grade 3',
            autoGenerated: true,
          },
          {
            type: 'CONFUSING',
            severity: 'LOW',
            description: 'Explanation may need simplification',
            autoGenerated: true,
          },
        ],
        reviewRequired: true,
        metadata: {
          assessmentId: 'qa-multi-flag-001',
          timestamp: new Date(),
          assessmentDuration: 300,
          modelVersion: 'qa-v1.0',
        },
      };

      expect(assessment.flags).toHaveLength(2);
      expect(assessment.reviewRequired).toBe(true);
      expect(assessment.flags[0].type).toBe('COMPLEX');
      expect(assessment.flags[1].severity).toBe('LOW');
    });

    it('should represent low-quality content requiring review', () => {
      const lowQualityAssessment: QualityAssessment = {
        confidenceScore: 0.55,
        appropriatenessScore: 0.6,
        educationalValue: 0.5,
        curriculumAlignment: 0.65,
        overallQuality: 0.58,
        flags: [
          {
            type: 'INACCURATE',
            severity: 'CRITICAL',
            description: 'Mathematical accuracy below threshold',
            autoGenerated: true,
          },
        ],
        reviewRequired: true,
      };

      expect(lowQualityAssessment.overallQuality).toBeLessThan(
        DEFAULT_QUALITY_THRESHOLDS.minimumOverallQuality
      );
      expect(lowQualityAssessment.reviewRequired).toBe(true);
      expect(lowQualityAssessment.flags[0].severity).toBe('CRITICAL');
    });
  });
});
